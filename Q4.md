M is the number of features
N is the number of samples
d is depth of the tree
The Theoretical time complexity for learning a decision tree is typically O(MNlog(N)).
This is because the algorithm must iterate through all samples and features to find the optimal split at each node of the tree. 
The Theoretical time complexity for prediction with a decision tree is O(d), as the algorithm must traverse the tree to reach a leaf node.
The depth of the tree is logarithmic in the number of samples time complexity for prediction becomes O(log(N))

Below are plots for time taken for training and prediction


Case 1: Discrete Input Real Output
![image](https://user-images.githubusercontent.com/90170940/214261466-4c7b1440-e60c-4388-882d-ae0a753386fe.png)
![image](https://user-images.githubusercontent.com/90170940/214261601-f70e3771-ffb2-456c-a658-2b88edc1832d.png)


Case 2: Discrete Input Discreate Output


![image](https://user-images.githubusercontent.com/90170940/214261850-6d4a42a5-adca-41c3-bf1a-66f8a2a73f2d.png)
![image](https://user-images.githubusercontent.com/90170940/214261937-d6afe1d7-f11d-46dd-9a13-8e8261390702.png)


Case 3: Real Input Real Output


![image](https://user-images.githubusercontent.com/90170940/214262351-729311be-c4c7-4f86-a702-4c89a4369672.png)
![image](https://user-images.githubusercontent.com/90170940/214262376-19a26de0-9d8b-4eb9-8f20-81a6eab34220.png)


Case 4: Real Input Discreate Output


![image](https://user-images.githubusercontent.com/90170940/214262918-6b0b734b-5ebd-4a31-b73b-4fe991412883.png)
![image](https://user-images.githubusercontent.com/90170940/214262948-8c5429a8-d83e-4c8c-a0df-bd31fd8670c7.png)


